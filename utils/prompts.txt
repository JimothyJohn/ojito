Create a prediction

 POST https://api.replicate.com/v1/predictions

Calling this operation starts a new prediction for the version and inputs you provide. As models can take several seconds or more to run, the output will not be available immediately. To get the final result of the prediction you should either provide a webhook URL for us to call when the results are ready, or poll the get a prediction endpoint until it has one of the terminated statuses.

Input and output (including any files) are automatically deleted after an hour, so you must save a copy of any files in the output if you'd like to continue using them.

Example request body:

{
  "version": "5c7d5dc6dd8bf75c1acaa8565735e7986bc5b66206b55cca93cb72c9bf15ccaa",
  "input": {
    "text": "Alice"
  }
}

Example cURL request (with your Default auth token included):

$ curl -s -X POST \
  -d '{"version": "5c7d5dc6dd8bf75c1acaa8565735e7986bc5b66206b55cca93cb72c9bf15ccaa", "input": {"text": "Alice"}}' \
  -H "Authorization: Token ccbbbaae4f65355e9f03d209dd968bf8410b3383" \
  -H 'Content-Type: application/json' \
  https://api.replicate.com/v1/predictions

Example response:

{
  "id": "ufawqhfynnddngldkgtslldrkq",
  "version": "5c7d5dc6dd8bf75c1acaa8565735e7986bc5b66206b55cca93cb72c9bf15ccaa",
  "urls": {
    "get": "https://api.replicate.com/v1/predictions/ufawqhfynnddngldkgtslldrkq",
    "cancel": "https://api.replicate.com/v1/predictions/ufawqhfynnddngldkgtslldrkq/cancel",
  },
  "created_at": "2022-04-26T22:13:06.224088Z",
  "status": "starting",
  "input": {
    "text": "Alice"
  },
  "error": null,
  "logs": "",
}

Request body

 input
The model's input as a JSON object. The input depends on what model you are running. To see the available inputs, click the "Run with API" tab on the model you are running. For example, stability-ai/stable-diffusion takes prompt as an input.

Files should be passed as data URLs or HTTPS URLs.

 stream
Request a URL to receive streaming output using server-sent events (SSE). If the requested model version supports streaming, then the returned prediction will have a stream entry in its urls property with a URL that you can use to construct an EventSource.

 version
The ID of the model version that you want to run.

 webhook
An HTTPS URL for receiving a webhook when the prediction has new output. The webhook will be a POST request where the request body is the same as the response body of the get prediction operation. If there are network problems, we will retry the webhook a few times, so make sure it can be safely called more than once.

 webhook_events_filter
By default, we will send requests to your webhook URL whenever there are new logs, new outputs, or the prediction has finished. You can change which events trigger webhook requests by specifying webhook_events_filter in the prediction request.

start: immediately on prediction start
output: each time a prediction generates an output (note that predictions can generate multiple outputs)
logs: each time log output is generated by a prediction
completed: when the prediction reaches a terminal state (succeeded/canceled/failed)
For example, if you only wanted requests to be sent at the start and end of the prediction, you would provide:

{
  "version": "5c7d5dc6dd8bf75c1acaa8565735e7986bc5b66206b55cca93cb72c9bf15ccaa",
  "input": {
    "text": "Alice"
  },
  "webhook": "https://example.com/my-webhook",
  "webhook_events_filter": ["start", "completed"]
}

Requests for event types output and logs will be sent at most once every 500ms. If you request start and completed webhooks, then they'll always be sent regardless of throttling.

Get a prediction

 GET https://api.replicate.com/v1/predictions/{prediction_id}

Returns the same response as the create a prediction operation. status will be one of:

starting: the prediction is starting up. If this status lasts longer than a few seconds, then it's typically because a new worker is being started to run the prediction.
processing: the predict() method of the model is currently running.
succeeded: the prediction completed successfully.
failed: the prediction encountered an error during processing.
canceled: the prediction was canceled by the user.
In the case of success, output will be an object containing the output of the model. Any files will be represented as URLs. You'll need to pass the Authorization header to request them.

In the case of failure, error will contain the error encountered during the prediction.

Terminated predictions (with a status of succeeded, failed or canceled) include a metrics object with a predict_time property showing the amount of CPU or GPU time, in seconds, that this prediction used while running. This is the time you're billed for, and it doesn't include time waiting for the prediction to start.

Example cURL request (with your Default auth token included):

$ curl -s \
-H "Authorization: Token ccbbbaae4f65355e9f03d209dd968bf8410b3383" \
https://api.replicate.com/v1/predictions/rrr4z55ocneqzikepnug6xezpe

The response is the prediction object:

{
  "id": "rrr4z55ocneqzikepnug6xezpe",
  "version": "be04660a5b93ef2aff61e3668dedb4cbeb14941e62a3fd5998364a32d613e35e",
  "urls": {
    "get": "https://api.replicate.com/v1/predictions/rrr4z55ocneqzikepnug6xezpe",
    "cancel": "https://api.replicate.com/v1/predictions/rrr4z55ocneqzikepnug6xezpe/cancel"
  },
  "created_at": "2022-09-13T22:54:18.578761Z",
  "started_at": "2022-09-13T22:54:19.438525Z",
  "completed_at": "2022-09-13T22:54:23.236610Z",
  "source": "api",
  "status": "succeeded",
  "input": {
    "prompt": "oak tree with boletus growing on its branches"
  },
  "output": [
    "https://replicate.com/api/models/stability-ai/stable-diffusion/files/9c3b6fe4-2d37-4571-a17a-83951b1cb120/out-0.png"
  ],
  "error": null,
  "logs": "Using seed: 36941...",
  "metrics": {
    "predict_time": 4.484541
  }
}

Request path parameters

 prediction_id
The ID of the prediction you want to get.